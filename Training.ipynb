{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: Muhammed Sinan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images is optional, CNNs are ok with large images\n",
    "SIZE_X = 512 #Resize images (height  = X, width = Y)\n",
    "SIZE_Y = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capture training image info as a list\n",
    "train_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory_path in glob.glob(\"/home/sinan/Master_thesis/python_project/new_data/images\"):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
    "        #print(img_path)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "        train_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert list to array for machine learning processing        \n",
    "train_images = np.array(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capture mask/label info as a list\n",
    "train_masks = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory_path in glob.glob(\"/home/sinan/Master_thesis/python_project/new_data/masks/\"):\n",
    "    for mask_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
    "        mask = cv2.imread(mask_path, 0)       \n",
    "        mask = cv2.resize(mask, (SIZE_Y, SIZE_X))\n",
    "        train_masks.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert list to array for machine learning processing          \n",
    "train_masks = np.array(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = train_masks/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use customary x_train and y_train variables\n",
    "X = train_images.astype(np.float)\n",
    "Y = train_masks.astype(np.float)\n",
    "#Y = np.expand_dims(Y, axis=3) #May not be necessary.. leftover from previous code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    us = keras.layers.UpSampling2D((2, 2))(x)\n",
    "    concat = keras.layers.Concatenate()([us, skip])\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet():\n",
    "    f = [16, 32, 64, 128, 256]\n",
    "    inputs = keras.layers.Input((SIZE_X , SIZE_Y, 3))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f[0]) #128 -> 64\n",
    "    c2, p2 = down_block(p1, f[1]) #64 -> 32\n",
    "    c3, p3 = down_block(p2, f[2]) #32 -> 16\n",
    "    c4, p4 = down_block(p3, f[3]) #16->8\n",
    "    \n",
    "    bn = bottleneck(p4, f[4])\n",
    "    \n",
    "    u1 = up_block(bn, c4, f[3]) #8 -> 16\n",
    "    u2 = up_block(u1, c3, f[2]) #16 -> 32\n",
    "    u3 = up_block(u2, c2, f[1]) #32 -> 64\n",
    "    u4 = up_block(u3, c1, f[0]) #64 -> 128\n",
    "    \n",
    "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"relu\")(u4)\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrics\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def Precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.math.sigmoid(y_pred)\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def f1_score_loss(y_true, y_pred):\n",
    "    f1_loss= K.binary_crossentropy(y_true, y_pred)+(1 - f1_score(y_true, y_pred))\n",
    "    return f1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "adam= tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-3,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-7,\n",
    "    amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import segmentation_models as sm\n",
    "\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "# focal_loss = sm.losses.BinaryFocalLoss()\n",
    "# total_loss = dice_loss + (1 * focal_loss)\n",
    "Jaccards_loss = sm.losses.JaccardLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "#model.load_weights(\"/home/sinan/Master_thesis/python_project/saved_model/saved_model\")\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNet()\n",
    "# model.compile(optimizer=\"adam\",\n",
    "#     loss=f1_score_loss, \n",
    "#     metrics=[\"accuracy\", tf.keras.metrics.MeanIoU(num_classes=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = [\n",
    "        #tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss'),\n",
    "        #tf.keras.callbacks.TensorBoard(log_dir='logs')]\n",
    "history=model.fit(x_train, \n",
    "          y_train,\n",
    "          batch_size=8, \n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          #callbacks=callbacks,       \n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = model.evaluate(x_val, y_val)\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig('/home/sinan/Master_thesis/python_project/graphs/C4_Binary_cross_entropy_with_aug_MIoU_Aug_9.png',dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['accuracy']\n",
    "val_loss = history.history['val_accuracy']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training accuracy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "#plt.savefig('/home/sinan/Master_thesis/python_project/graphs/C4_Binary_cross_entropy_accuracy_with_aug_MIoU_Aug_9.png',dpi=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['mean_io_u']\n",
    "val_loss = history.history['val_mean_io_u']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training MIoU')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation MIoU')\n",
    "plt.title('Training and validation MIoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MIoU')\n",
    "plt.legend()\n",
    "#plt.savefig('/home/sinan/Master_thesis/python_project/graphs/C4_Binary_cross_entropy_MIoU_with_aug_MIoU_Aug_9.png',dpi=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_img = cv2.imread('/home/sinan/Master_thesis/python_project/sinan/train/train1.jpeg', cv2.IMREAD_COLOR)       \n",
    "test_img = cv2.resize(test_img, (SIZE_Y, SIZE_X))\n",
    "test_img = cv2.cvtColor(test_img, cv2.COLOR_RGB2BGR)\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "test_img = np.expand_dims(test_img, axis=0)\n",
    "\n",
    "prediction = model.predict(test_img)\n",
    "\n",
    "#plt.savefig('/home/sinan/Master_thesis/python_project/prediction_image/train_15.png',dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View and Save segmented image\n",
    "prediction_image = prediction.reshape(mask)\n",
    "plt.imshow(prediction_image,cmap='gray')\n",
    "#plt.imsave('images/test_images/segmented.jpg', prediction_image, cmap='gray')\n",
    "#plt.savefig('/home/sinan/Master_thesis/python_project/prediction_image/predicted_15.png',dpi=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"/home/sinan/Master_thesis/python_project/saved_model/saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('/home/sinan/Master_thesis/python_project/models/C4_Binary_cross_entropy_with_MIoU_aug_Aug_9.h5')\n",
    "#model.save(\"saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# model = keras.models.load_model('/home/sinan/Master_thesis/python_project/models/C4_Binary_cross_entropy_with_aug_Aug_9.hdf5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #View and Save segmented image\n",
    "# prediction_image = prediction.reshape(shape_mask)\n",
    "# plt.imshow(prediction_image, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
